{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a14866fcdd9d4cc1ba50b41537577ac1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d65bf9eab0c47e280355e5a50f225b2",
              "IPY_MODEL_3e5109930a8d474db70fbfc1acd84487",
              "IPY_MODEL_2913c2b779114fbb83679f6d015bef93"
            ],
            "layout": "IPY_MODEL_94152ce4713745b8bdc14170b33af432"
          }
        },
        "9d65bf9eab0c47e280355e5a50f225b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69add5ced3b840f690d82ca7b9df5cf6",
            "placeholder": "​",
            "style": "IPY_MODEL_2d1212d6f2554b1eb9181eac60b0e8b6",
            "value": "100%"
          }
        },
        "3e5109930a8d474db70fbfc1acd84487": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ad94e67c1754a07aff79908505236b0",
            "max": 167502836,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0f96e0726f04e8892ddd8bed5f039b2",
            "value": 167502836
          }
        },
        "2913c2b779114fbb83679f6d015bef93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87e628b66ec34fbd82f4af10d070bb1d",
            "placeholder": "​",
            "style": "IPY_MODEL_a75741d4a77f40e081db998028afdf18",
            "value": " 160M/160M [00:02&lt;00:00, 68.2MB/s]"
          }
        },
        "94152ce4713745b8bdc14170b33af432": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69add5ced3b840f690d82ca7b9df5cf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d1212d6f2554b1eb9181eac60b0e8b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ad94e67c1754a07aff79908505236b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0f96e0726f04e8892ddd8bed5f039b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "87e628b66ec34fbd82f4af10d070bb1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a75741d4a77f40e081db998028afdf18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Title : Disease Detection using Faster R-CNN with Python\n",
        "---"
      ],
      "metadata": {
        "id": "O7rL77wG06DB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 0. 환경설정"
      ],
      "metadata": {
        "id": "MJprZ9jt0t95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TSeOHLhoojp",
        "outputId": "c8969410-b5fb-4a78-f08b-1c13b8bfe53f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU 확인. 코랩에서는 랜덤 GPU할당되므로 메모리 부족현상 발생 가능 -> GPU확인 후 메모리 충분할 경우 진행 권장\n",
        "\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf4yxe4Z08yh",
        "outputId": "76600076-631c-44dc-9c1a-2d425821ac20"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla V100-SXM2-16GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 임포트\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import shutil as sh\n",
        "\n",
        "# 바운딩박스 도식화 위한 패키지 임포트\n",
        "import glob  # 파일, 디렉토리 처리\n",
        "import matplotlib.pyplot as plt  \n",
        "import matplotlib.image as mpimg  # matplotlib 시각화\n",
        "import matplotlib.patches as patches\n",
        "from bs4 import BeautifulSoup  # HTML, XML 파일 파싱, 웹스크래핑 \n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torchvision  # 이미지 처리를 위해 사용되며 데이터셋에 관한 패키지와 모델에 관한 패키지 내장\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import albumentations\n",
        "import albumentations.pytorch\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "from torchvision import transforms, datasets, models\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
      ],
      "metadata": {
        "id": "DVjr_fK21Nvh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 1. 데이터 준비\n",
        "\n",
        "* 플젝(농작물 병해진단) 데이터셋은 마스크데이터 데이터셋과 다섯 가지 측면에서 차이가 있음\n",
        "    * 데이터셋 폴더 구조가 다름\n",
        "    * annotation 파일이 xlm파일이 아니고 json파일인 점\n",
        "    * bbox 좌표 형식이 다름xml(x1,y1,x2,y2)->json(x,y,h,w)\n",
        "    * label로 쓸 수 있는 직접적인 항목이 없음\n",
        "    * 마스크검출에서 바운딩 박스, 레이블은 검출된 객체인 N개의 마스크에 대해서만 annotation에 있는 반면에, 플젝데이터셋의 annotation에는 이미지당 1개씩의 객체와 레이블(로 조합해 활용할 수 있는 항목들)이 있고, 병징을 나타내는 N개의 part에 대한 바운딩박스가 있다는 점에서 차이가 있음.\n",
        "\n",
        "=> 각각에 대한 처리 방법은\n",
        "1. 플젝의 데이터 구조를 그냥 두고 필요한 파일을 불러와서 처리하는 방식도 가능하겠지만, 혼란을 줄이고 처리를 용이하게 하기 위해서 디렉토리 구조를 마스크데이터셋처럼 맞춰줌\n",
        "2. xml파일 대신 json파일에서 필요 정보 파싱해서 사용하면 됨\n",
        "3. 상이한 좌표형식을 환산식을 사용해 처리\n",
        "4. label로 쓸 수 있는 항목을 추출해 label로 쓸 수 있는 코드를 생성해 json 파일에 추가함\n",
        "*5.(이 부분이 핵심임) 병징을 나타내는 N개의 정보(바운딩박스좌표, id)를 어떻게 활ㄹ용할 것인가?         "
      ],
      "metadata": {
        "id": "E4U65Y861V7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### S1-1P. 데이터 불러오기(농작물병해)"
      ],
      "metadata": {
        "id": "b-DByTbTZbEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 플젝 데이터셋에서 데이터 불러와 확인을 위한 테스트 부분임\n",
        "\n",
        "# %cd /content/drive/MyDrive/aiffelthon/PjtDataset/train\n",
        "# !pwd\n",
        "\n",
        "# train = glob.glob('*')\n",
        "# print(\"train:\",train)\n",
        "# print(len(train))\n",
        "# print(train[40])\n",
        "# sample = glob.glob('train')\n",
        "# print(len(sample))\n",
        "\n",
        "# # train_image_sample = cv2.imread(glob.glob('train[40]'+'/*.jpg')[0])\n",
        "# train_json_sample = json.load(open(glob.glob(train[40]+'/*.json')[0], 'r'))\n",
        "\n",
        "# %cd /content/drive/MyDrive/aiffelthon/PjtDataset/train\n",
        "# !pwd\n",
        "# f_names = glob.glob('*')\n",
        "# print(len(f_names))\n",
        "# sample = f_names[40]\n",
        "\n",
        "# sample_train_image = cv2.imread(glob.glob(sample+'/*.jpg')[0])\n",
        "# print(sample_train_image)\n",
        "# # train_json_sample = json.load(open(glob.glob(train[42]+'/*.json')[0], 'r'))\n",
        "\n",
        "# with open((sample_train_image, \"r\") as json_file:\n",
        "#     sample_train_json = json.load(open(glob.glob(sample+'/*.json')[0], 'r'))\n",
        "#     sample_train_json['annotations'].append({\n",
        "#     \"label\": \"1_00_0\"\n",
        "#     })"
      ],
      "metadata": {
        "id": "BZV7fnf7ZcAz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 플젝 데이터셋에서 데이터 불러와 확인을 위한 테스트 부분임 : 이미지 파일 확인\n",
        "\n",
        "# # image\n",
        "# plt.imshow(cv2.cvtColor(train_image_sample, cv2.COLOR_BGR2RGB))\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "xdtOgbaCZcE5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 플젝 데이터셋에서 데이터 불러와 확인을 위한 테스트 부분임 : json파일 확인\n",
        "# # json\n",
        "# print(train_json_sample)\n",
        "# print(train_json_sample['annotations']['area'])"
      ],
      "metadata": {
        "id": "NCLXnCc1ZcIK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### S1-2P.바운딩박스\n",
        "* 플젝 baseline 코드에서 바운딩박스를 쳐주는 코드 발췌해 온 것임"
      ],
      "metadata": {
        "id": "hFMnsGZ0aOzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 플젝 baseline 코드에서 바운딩박스를 쳐주는 코드 발췌해 온 것임\n",
        "# # visualize bbox\n",
        "# plt.figure(figsize=(7,7))\n",
        "# points = sample_json['annotations']['bbox'][0]\n",
        "# part_points = sample_json['annotations']['part']\n",
        "# img = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# cv2.rectangle(\n",
        "#     img,\n",
        "#     (int(points['x']), int(points['y'])),\n",
        "#     (int((points['x']+points['w'])), int((points['y']+points['h']))),\n",
        "#     (0, 255, 0),\n",
        "# )\n",
        "# for part_point in part_points:\n",
        "#     point = part_point\n",
        "#     cv2.rectangle(\n",
        "#         img,\n",
        "#         (int(point['x']), int(point['y'])),\n",
        "#         (int((point['x']+point['w'])), int((point['y']+point['h']))),\n",
        "#         (255, 0, 0),\n",
        "#         1\n",
        "#     )\n",
        "# plt.imshow(img)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "SWsCQk9cZcMW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### S1-3P. label 만들기\n"
      ],
      "metadata": {
        "id": "XJ1sWqmyaFgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# label 생성을 위해 개별 항목에 대한 코드(딕셔너리)화 작업.\n",
        "# 변수 설명 csv 파일 참조\n",
        "\n",
        "crop = {'1':'딸기','2':'토마토','3':'파프리카','4':'오이','5':'고추','6':'시설포도'}\n",
        "\n",
        "disease = {'1':{'a1':'딸기잿빛곰팡이병','a2':'딸기흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
        "           '2':{'a5':'토마토흰가루병','a6':'토마토잿빛곰팡이병','b2':'열과','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
        "           '3':{'a9':'파프리카흰가루병','a10':'파프리카잘록병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
        "           '4':{'a3':'오이노균병','a4':'오이흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
        "           '5':{'a7':'고추탄저병','a8':'고추흰가루병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
        "           '6':{'a11':'시설포도탄저병','a12':'시설포도노균병','b4':'일소피해','b5':'축과병'}}\n",
        "\n",
        "risk = {'1':'초기','2':'중기','3':'말기'}"
      ],
      "metadata": {
        "id": "4z9KNrlbaFmu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label 생성 : 개별 항목에 대한 코드를 이용해 '작물_병해_진행단계' 형식으로 label 생성\n",
        "\n",
        "label_description = {}\n",
        "for key, value in disease.items():\n",
        "    label_description[f'{key}_00_0'] = f'{crop[key]}_정상'\n",
        "    for disease_code in value:\n",
        "        for risk_code in risk:\n",
        "            label = f'{key}_{disease_code}_{risk_code}'\n",
        "            label_description[label] = f'{crop[key]}_{disease[key][disease_code]}_{risk[risk_code]}'\n",
        "list(label_description.items())[:10]\n",
        "print(len(label_description))"
      ],
      "metadata": {
        "id": "G83CS3s_aFvl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb507cb4-8ba9-4c41-8eb8-0791c5e0aebb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# label 생성 : 생성한 label를 처리할 때 사용할 수 있게 encoder, decoder를 만들어둠\n",
        " \n",
        "label_encoder = {key:idx for idx, key in enumerate(label_description)}\n",
        "label_decoder = {val:key for key, val in label_encoder.items()}"
      ],
      "metadata": {
        "id": "srio3UyFll9t"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(label_encoder['1_00_0'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ6XpU7R5btw",
        "outputId": "afdbf1d3-25f9-4b4c-c83f-e50bbb28a160"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### S1-1B. 데이터 불러오기(마스크 데이터셋)"
      ],
      "metadata": {
        "id": "d44AaCCv1zFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 데이터로더 파일이 들어 있는 유틸리티 파일을 clone하고\n",
        "# !git clone https://github.com/Pseudo-Lab/Tutorial-Book-Utils\n",
        "\n",
        "# # 데이터로더 파일을 실행시켜 데이터셋을 다운로드 받음. --data 옵션에 FaceMaskDetection 파라미터를 줌\n",
        "# !python Tutorial-Book-Utils/PL_data_loader.py --data FaceMaskDetection"
      ],
      "metadata": {
        "id": "knYkl5rS134D"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 다운받은 파일 압축 풀기\n",
        "\n",
        "# !unzip -q Face\\ Mask\\ Detection.zip"
      ],
      "metadata": {
        "id": "3ycKU3YV2BPz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### S1-2. 데이터 분리 => 데이터셋 폴더구조 변경\n",
        "* 데이터셋에서 훈련셋과 시험셋을 나눠줌\n",
        "* 일반적으로 훈련셋:시험셋은 7:3으로 나누지만, 데이터셋이 적을 때는 8:2로 나눠주기도 함\n",
        "=> \n",
        "* 플젝 데이터셋은 이미 test와 train 폴더로 나눠져 있기 때문에 이 작업은 필요하지 않음\n",
        "* 다만, 마스크 데이터셋의 폴더구조는\n",
        "\n",
        "    FaceMask Dataset\n",
        "    - imges\n",
        "        -   .png\n",
        "    - annotations\n",
        "        -   .xml\n",
        "    => 인 것을 코드 내에서 test, train 으로 구분해 줌\n",
        "    - image\n",
        "    - test_image\n",
        "    - annotations\n",
        "    - test_annotations\n",
        "\n",
        "* 플젝 데이터셋의 폴더구조는 다음과 같이 되어 있어서 이를 FaceMask 데이터셋의 폴더구조와 같게 맞춰주었음\n",
        "\n",
        "    Pjtdataset(농작물 병해 데이터셋)\n",
        "    - train\n",
        "        - 10028\n",
        "            -   .jpg\n",
        "            -   .json\n",
        "            -   .csv\n",
        "    - test\n",
        "        - 10000\n",
        "            -   .jpg\n",
        "\n",
        "    => 별도의 작업을 통해 다음과 같이 폴더구조를 만들어 줌\n",
        "\n",
        "    Pjtdataset(농작물 병해 데이터셋)\n",
        "        - train\n",
        "            - images\n",
        "                -   .jpg\n",
        "            - annotations\n",
        "                -   .json\n",
        "        - test\n",
        "            - images\n",
        "                -   .jpg\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JtNcB3_X2XSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 데이터셋을 불러서 학습셋과 시험셋으로 분리해 줌\n",
        "\n",
        "# # 데이터의 갯수를 확인해 봄\n",
        "# print(len(os.listdir('annotations')))\n",
        "# print(len(os.listdir('images')))\n",
        "\n",
        "# # 테스트셋을 저장할 폴더를 만들어줌\n",
        "# !mkdir test_images\n",
        "# !mkdir test_annotations\n",
        "\n",
        "# # 랜덤하게 수를 발생시켜 인덱스로 사용하고, random.sample()함수를 사용해 훈련셋과 시험셋으로 나눔\n",
        "# random.seed(1234)\n",
        "# idx = random.sample(range(853), 170)\n",
        "\n",
        "# for img in np.array(sorted(os.listdir('images')))[idx]:\n",
        "#     shutil.move('images/'+img, 'test_images/'+img)\n",
        "\n",
        "# for annot in np.array(sorted(os.listdir('annotations')))[idx]:\n",
        "#     shutil.move('annotations/'+annot, 'test_annotations/'+annot)\n",
        "\n",
        "# # 나눈 후 각각의 폴더에 있는 데이터 갯수를 확인해 봄\n",
        "# print(len(os.listdir('annotations')))\n",
        "# print(len(os.listdir('images')))\n",
        "# print(len(os.listdir('test_annotations')))\n",
        "# print(len(os.listdir('test_images')))"
      ],
      "metadata": {
        "id": "LvWKYEe22ywu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### S1-X. json파일 읽어서 label항목을 만들어 추가해 줌"
      ],
      "metadata": {
        "id": "Bp_O6BL1ikfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import glob\n",
        "# train_annot_list = glob.glob('/content/drive/MyDrive/aiffel/Datathon/data/train/annotations/*')\n",
        "# print(train_annot_list)"
      ],
      "metadata": {
        "id": "lpK0YiuAil9_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# json파일에 있는 작물(crop), 병해(disease), 상태(risk) 정보를 파싱해서 \n",
        "# label로 쓸 코드를 만들어 주는 코드 => 작업 완료\n",
        "\n",
        "# import json\n",
        "# for i, annot in enumerate(train_annot_list):\n",
        "\n",
        "#     # if i == 10 :\n",
        "#     #     break\n",
        "#     with open(annot, \"r\") as file:\n",
        "#         j_file = json.load(file)\n",
        "#     # print(j_file)\n",
        "\n",
        "#     label = str(j_file['annotations']['crop'])+\"_\"+str(j_file['annotations']['disease'])+\"_\"+str(j_file['annotations']['risk'])\n",
        "#     print('i : ',i, ', label : ',label)\n",
        "#     j_file['annotations']['label']=label \n",
        "#     # # j_file   \n",
        "#     with open(annot, \"w\") as file:\n",
        "#         json.dump(j_file, file)\n"
      ],
      "metadata": {
        "id": "e-BljGAoi1Pw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # json파일 열어보기\n",
        "\n",
        "# with open('/content/drive/MyDrive/aiffelthon/PjtDataset/train/annotations/58464.json', \"r\") as file:\n",
        "#     sample = json.load(file)\n",
        "# sample"
      ],
      "metadata": {
        "id": "T1vi4IRQi1S2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 병해객체 바운딩박스 인덱싱 확인\n",
        "\n",
        "# print(sample['annotations']['bbox'][0]['x'])"
      ],
      "metadata": {
        "id": "qZBowDPIi1Vw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 병징(part) 바운딩박스 인덱싱 확인\n",
        "\n",
        "# print(sample['annotations']['part'][0]['x'])"
      ],
      "metadata": {
        "id": "ioCJ_GSfi1YR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-gAcvc2yi1bo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "84Y3BGisi1em"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### S1-3. 바운딩박스 관련 함수 정의\n"
      ],
      "metadata": {
        "id": "TZXEieNP3E7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 바운딩박스 관련 함수 내부 코드 작동 확인을 위한 부분임\n",
        "\n",
        "# with open('/content/drive/MyDrive/aiffelthon/PjtDataset/train/19011/19011.json') as f:\n",
        "\n",
        "#     # json파일을 읽어서 part\n",
        "#     data = json.load(f)  # json파일을 읽어서\n",
        "#     # soup = BeautifulSoup(data, \"html.parser\")\n",
        "#     # objects = soup.find_all(\"object\")  # object 항목들이 무엇인지 확인 요\n",
        "#     print(data)\n",
        "#     objects = data['annotations']['bbox']\n",
        "#     print(objects)\n",
        "# # 함수 내부 코드 확인을 위한 코드임\n",
        "\n",
        "#     num_part = len(data['annotations']['bbox'])  # objects의 갯수를 할당해서 \n",
        "#     print(num_part)\n",
        "\n",
        "# boxes = generate_box(sample)\n",
        "\n",
        "# pboxes = []\n",
        "\n",
        "# for i in range(num_part):\n",
        "#     # labels.append(generate_label(i))\n",
        "#     # boxes.append(generate_box(i))\n",
        "#     pboxes.append(generate_pbox(sample,i))\n",
        "# labels = generate_label(sample)\n",
        "# labels = torch.as_tensor(labels, dtype=torch.int64) \n",
        "# boxes = torch.as_tensor(boxes, dtype=torch.float32) \n",
        "# pboxes = torch.as_tensor(pboxes, dtype=torch.float32) \n",
        "\n",
        "# target = {}\n",
        "# target[\"labels\"] = labels\n",
        "# target[\"boxes\"] = boxes\n",
        "# target[\"pboxes\"] = pboxes\n",
        "\n",
        "# print(target)"
      ],
      "metadata": {
        "id": "eIFKlG31eEXb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # generate_label()함수 내용 확인을 위한 부분임\n",
        "# with open('/content/drive/MyDrive/aiffelthon/PjtDataset/train/annotations/19667.json', \"r\") as f:\n",
        "#     data = json.load(f)\n",
        "\n",
        "#     objects = data['annotations']\n",
        "#     # print(objects)\n",
        "#     # print(len(objects))\n",
        "#     boxes = []\n",
        "#     for i in objects :\n",
        "#         label = objects['label']\n",
        "#         # print(label)\n",
        "#         label = label_encoder[label]\n",
        "\n",
        "#     # print(objects)\n",
        "#     # label = obj['annotations']['label']\n",
        "#     # print(label)\n",
        "#     # label = label_encoder[label]\n"
      ],
      "metadata": {
        "id": "3Ax9JUVOSuX1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 바운딩박스를 위한 함수들 정의\n",
        "\n",
        "# 병해 객체에 대한 바운딩박스를 쳐주는 함수. 병해 객체는 하나의 이미지에 한 개만 있음\n",
        "def generate_box(obj):\n",
        "    '''\n",
        "    annotation 파일에서 객체 bouding box 좌표를 읽어서 리턴함\n",
        "    사용하는 bbox좌표 형식은 (좌상x, 좌상y, 우하x, 우하y)\n",
        "    '''\n",
        "    \n",
        "    xmin = float(obj['annotations']['bbox'][0]['x'])  #find('xmin').text)\n",
        "    ymin = float(obj['annotations']['bbox'][0]['y'])  #find('ymin').text)\n",
        "    xmax = float(obj['annotations']['bbox'][0]['x']) + float(obj['annotations']['bbox'][0]['w'])  #find('xmax').text)\n",
        "    ymax = float(obj['annotations']['bbox'][0]['y']) + float(obj['annotations']['bbox'][0]['h'])  #find('ymax').text)\n",
        "\n",
        "    return [xmin, ymin, xmax, ymax]\n",
        "\n",
        "'''\n",
        "병징 관련된 부분(part)은 일단 배제\n",
        "* 주의사항 : part(병징)정보가 없는 경우도 꽤 있음...추후 코드 구현시 참고바람\n",
        "\n",
        "# N개의 병징들에 대해 바운딩박스를 쳐주는 함수 : 추천 코드에 없는데 추가한 부분임\n",
        "# N개의 병징들은 \n",
        "\n",
        "def generate_pbox(obj,i):\n",
        "    \n",
        "    #annotation 파일에서 질병(part) bouding box 좌표를 읽어서 리턴함\n",
        "    #사용하는 bbox좌표 형식은 (좌상x, 좌상y, 우하x, 우하y, id)\n",
        "    \n",
        "    \n",
        "    xmin_p = float(obj['annotations']['part'][i]['x'])  #find('xmin').text)\n",
        "    ymin_p = float(obj['annotations']['part'][i]['y'])  #find('ymin').text)\n",
        "    xmax_p = float(obj['annotations']['part'][i]['x']) + float(obj['annotations']['part'][i]['w'])  #find('xmax').text)\n",
        "    ymax_p = float(obj['annotations']['part'][i]['y']) + float(obj['annotations']['part'][i]['h'])  #find('ymax').text)\n",
        "    # id_p   = obj['annotations']['part'][i]['id']\n",
        "\n",
        "    return [xmin_p, ymin_p, xmax_p, ymax_p]\n",
        "\n",
        "adjust_label = 1  # adjust_lable : ??? <= 이게 왜 필요할까요?\n",
        "'''\n",
        "\n",
        "# 레이블을 생성해 주는 함수. 병해를 나타내는 레이블은 하나의 이미지당 1개임\n",
        "# 마스크 디텍션에서 레이블은 3가지 경우의 수가 있고 if문으로 처리하였으나\n",
        "# 농작물 병해는 작물_병해_진행단계 를 조합하여 만들어진 135개의 레이블이 있어\n",
        "# 이를 처리해 주기 위해 encoder, decoder로 만들어 활용함\n",
        "\n",
        "def generate_label(obj):  \n",
        "    '''\n",
        "    마스크착용상태 읽어서 레이블(정상착용, 비정상착용, 미착용)을 숫자로 리턴\n",
        "    => json파일의 annotations에 추가한 label에서 읽어와서 할당하고 리턴함\n",
        "    '''\n",
        "    # print(obj)\n",
        "    label = obj['annotations']['label']\n",
        "    # print(label)\n",
        "    label = label_encoder[label]\n",
        "\n",
        "    return label\n",
        "    # return 0 + adjust_label\n",
        "\n",
        "# 이 함수가 대회용 baseline 모델에는 없는 부분이므로 내용 파악 잘 해야함\n",
        "def generate_target(file): \n",
        "    '''\n",
        "    generate_box(), generate_label()를 각각 호출해서 반환된 값을 딕셔너리에 저장해 반환\n",
        "    html.parser()를 이용해 annotatin에 있는 내용을 불러와 타켓의 바운딩박스와 라벨에 추가함\n",
        "    '''\n",
        "\n",
        "    '''\n",
        "    json파일을 읽어들여서\n",
        "    한 개의 label과\n",
        "    한 개의 대상객체 바운딩박스bbox와\n",
        "    병해증상(part)의 갯수를 파악해서\n",
        "        여러개의 병해증상 바운딩박스pbbox 를 리스트에 담은 후\n",
        "    tensor에 할당하고\n",
        "    target딕셔너리의 각 키값에 할당한 뒤\n",
        "    리턴 \n",
        "    '''\n",
        "\n",
        "    with open(file) as f:\n",
        "\n",
        "        # json파일을 읽어서 part\n",
        "        data = json.load(f)  # json파일을 읽어서\n",
        "        # soup = BeautifulSoup(data, \"html.parser\")\n",
        "        # objects = soup.find_all(\"object\")  # object 항목들이 무엇인지 확인 요\n",
        "\n",
        "        # 추천 코드에서와 마찬가지로 객체(여기서는 병해작물)를 objects로 할당\n",
        "        # 그런데...마스크 디텍션에 쓰는 xml에서는 각각의 object 리스트 안에 \n",
        "        # label('name')과 bbox좌표 등의 정보가 포함되어 있기 때문에 objects를 \n",
        "        # get_boxe()나 get_label()의 매개변수로 넘겨주는 형식을 취함. \n",
        "        # 단, for문을 사용해 i로 넘겨주고 함수에서는 obj로 받는데..\n",
        "        # 이 때 obj는 object 리스트임\n",
        "\n",
        "        # 그런데... 농작물병해데이터셋 json에서는..annotation 아래로 bbox와 \n",
        "        # label(생성해 준) 정보가 있는 점이 다름... 따라서... objects를 json에서\n",
        "        # 어떤 단위에서 잡아줄 지 주의해야 하고, 함수에서 받았을 때 파싱에 유의해야 함  \n",
        "        objects = data['annotations']\n",
        "\n",
        "        '''\n",
        "        일단 part에 대해 추가했던 이 부분도 배제\n",
        "\n",
        "        # 마스크 디텍션 코드에서 objects 는 label이 붙는 검출된 다수의 마스크만을 의미하지만\n",
        "        # 농작물 병해진단에서는 label이 붙는 객체는 1개이며, 대신 병징을 나타내는 여러개의 part로 구성됨\n",
        "        num_part = len(data['annotations']['part'])  # objects(part)의 갯수를 할당해서 \n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        대신 추천코드에서 처럼 object(마스크 디텍션에서는 N개의 마스크)의 갯수를 찾아서 \n",
        "        그 갯수만큼 처리해주는 방식으로 해 봄.. 병해는 이미지에 하나지만...\n",
        "        '''\n",
        "\n",
        "        num_objs = len(data['annotations']['bbox'])\n",
        "\n",
        "        # 대상 객체를 나타내는 boxes와 label\n",
        "        # 각 함수의 리턴값은 리스트 형태임\n",
        "        labels = []\n",
        "        boxes = []\n",
        "\n",
        "        for i in objects :\n",
        "            labels.append(generate_label(data)) \n",
        "            boxes.append(generate_box(data))\n",
        "        \n",
        "        '''\n",
        "        # 병징을 나타내는 part들에 대한 좌표를 저장할 변수를 리스트로 선언해 주고\n",
        "        pboxes = []\n",
        "\n",
        "        # part(병징)의 수 만큼 for문을 돌려서 좌표를 구해서 pboxes리스트에 추가해줌\n",
        "        for i in range(num_part):\n",
        "            # labels.append(generate_label(i))\n",
        "            # boxes.append(generate_box(i))\n",
        "            pboxes.append(generate_pbox(data,i))\n",
        "        '''\n",
        "        # 저장된 labels, boxes, pboxes를 tensor로 저장\n",
        "        # 일단, 원래 코드에 있는 데이터사이즈로 저장해 줌 => 이게 문젠가??\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64) \n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32) \n",
        "        '''\n",
        "        pboxes = torch.as_tensor(pboxes, dtype=torch.float32) \n",
        "        '''\n",
        "        # 함수의 리턴값인 target에 패킹해줌\n",
        "        target = {}\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"boxes\"] = boxes \n",
        "        # 멘토님 추천 코드 : boxes.unsqueeze(dim=0) # (4) -> (N=1, 4)\n",
        "        # 위에서 labels을 리스트로 선언해주고 함수에서 리턴받은 리스트값을 추가해 줌\n",
        "        '''\n",
        "        target[\"pboxes\"] = pboxes\n",
        "        '''\n",
        "        return target\n",
        "\n",
        "def plot_image_from_output(img, annotation):\n",
        "    '''\n",
        "    기존 마스크검출 코드의 내용임.\n",
        "    이미지와 바운딩박스 함께 시각화\n",
        "    마스크 착용시 green, 미 착용시 red, 잘못 착용시 orange\n",
        "    \n",
        "    데이터 탐색에서의 plot_image()는 이미지를 파일경로에서 읽어오지만\n",
        "    plot_image_from_output()은 torch.tensor로 변환된 이미지를 시각화함\n",
        "    '''\n",
        "    \n",
        "    # img = mping.imread(img_path) \n",
        "    img = img.cpu().permute(1,2,0)\n",
        "\n",
        "    '''\n",
        "    pytorch에서는 이미지를 [channel, height, width]형식으로 표현하는 반면\n",
        "    matplotlib에서는 [height, width, channel]형식으로 표현하므로\n",
        "    채널 순서를 바꿔주는 permute()함수를 활용해서 matplotlib에서 사용하기 위해\n",
        "    채널 순서를 바꿔줌\n",
        "    '''\n",
        "\n",
        "    fig,ax = plt.subplots(1)\n",
        "    ax.imshow(img)\n",
        "    \n",
        "    '''\n",
        "    annotation에서 bbox 좌표를 받아서 대상객체 바운딩 박스를 그려준다.\n",
        "    '''\n",
        "    xmin, ymin, xmax, ymax = annotation[\"boxes\"][idx].cpu()  ## 원래 코드 -> 플젝코드로 바꿔줘야 함\n",
        "    rect_b = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=2,edgecolor='r',facecolor='none')\n",
        "  \n",
        "    ax.add_patch(rect_b)\n",
        "    # ??? add_patch() 함수는 좌상x, 좌상y, h, w를 좌표로 해서 그림을 그려주는데...???\n",
        "    # 아항.. w, h를 각각 Xmax - Xmin, Ymax-Ymin 으로 환산해서 값을 구해 적용해 주네 ㅋ\n",
        "    # 그러면.. 애초에 json에서는 x, y, h, w로 좌표가 주어지니까 굳이 환산할 필요가 없네 ㅠㅠ\n",
        "    # 요 부분은 나중에 ㅋ\n",
        "    '''\n",
        "    annotation에서 pbox들의 좌표를 받아서 여러개의 병해부위 바운딩 박스를 그려준다\n",
        "    '''\n",
        "\n",
        "    for idx in range(len(annotation[\"pboxes\"])):\n",
        "        xmin, ymin, xmax, ymax = annotation[\"pboxes\"][idx].cpu()  \n",
        "            # .cpu()를 해주지 않으면 tensor관련 에러가 발생함\n",
        "\n",
        "        rect_p = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='orange',facecolor='none')\n",
        "        \n",
        "        ax.add_patch(rect_p)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Sam9tsxw3X6p"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### S1-4. 데이터셋 클래스와 데이터 로더 정의\n",
        "\n",
        "* 파이토치 모델을 학습시키기 위해서 데이터셋 클래스를 정의해야 함\n",
        "\n",
        "* torchvision에서 제공하는 객체탐지모델을 학습시키기 위한 데이터셋클래스의 __getitem__메서드는 이미지 파일과 바운딩박스 좌표를 반환함\n",
        "\n",
        "* 최종적으로 훈련용 데이터와 시험용 데이터를 batch단위로 불러올 수 있게 torch.utils.data.DataLoader함수를 활용해 data_loader와 test_data_loader를 각각 정의함\n",
        "\n",
        "* torch.utils.data.DataLoader() 함수를 통해 배치 사이즈를 4로 지정해 불러옴\n",
        "* 배치 사이즈는 개인의 메모리 크기에 따라 자유롭게 설정하면 됨"
      ],
      "metadata": {
        "id": "_03Rwvvq3eeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # sorted(os.listdir(path))) 동작 확인\n",
        "\n",
        "# # path = '/content/drive/MyDrive/aiffelthon/PjtDataset/train/images'  # /content/drive/MyDrive/aiffelthon/PjtDataset/train\n",
        "# # print(list((sorted(os.listdir(path)))))\n",
        "\n",
        "# %cd /content/drive/MyDrive/aiffelthon/PjtDataset/test\n",
        "# !pwd\n",
        "# lists = glob.glob('./test_images_back')\n",
        "# # for i, list in enumerate(lists):\n",
        "# #     print('list: ',list)\n",
        "#     # sh.copy('./test_images_bak/list','./test_images/')\n",
        "#     # if i > 2000:\n",
        "#     #     break"
      ],
      "metadata": {
        "id": "6M2WyBCZCiST"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskDataset(object):\n",
        "    def __init__(self, transforms, path):\n",
        "        '''\n",
        "        path: path to train folder or test folder\n",
        "        '''\n",
        "        # transform module과 img path 경로를 정의\n",
        "        self.transforms = transforms\n",
        "        self.path = path\n",
        "        self.imgs = list(sorted(os.listdir(self.path)))\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx): #special method\n",
        "        # load images ad masks\n",
        "        file_image = self.imgs[idx]\n",
        "        file_label = self.imgs[idx][:-3] + 'json'\n",
        "        img_path = os.path.join(self.path, file_image)\n",
        "        \n",
        "        if 'test' in self.path:\n",
        "            label_path = os.path.join(\"/content/drive/MyDrive/aiffelthon/PjtDataset/test_annotations/\", file_label)\n",
        "        else:\n",
        "            label_path = os.path.join(\"/content/drive/MyDrive/aiffelthon/PjtDataset/annotations/\", file_label)\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Generate Target\n",
        "        target = generate_target(label_path)\n",
        "        \n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        # print(img.shape)\n",
        "        # print(target.shape)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self): \n",
        "        return len(self.imgs)\n",
        "\n",
        "data_transform = transforms.Compose([  # transforms.Compose : list 내의 작업을 연달아 할 수 있게 호출하는 클래스\n",
        "        transforms.ToTensor() # ToTensor : numpy 이미지에서 torch 이미지로 변경\n",
        "    ])\n",
        "'''\n",
        "추후에 데이터전처리와 관련된 작업 내용을 Compose에 추가해야 함\n",
        "'''\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "dataset = MaskDataset(data_transform, '/content/drive/MyDrive/aiffelthon/PjtDataset/images/')\n",
        "test_dataset = MaskDataset(data_transform, '/content/drive/MyDrive/aiffelthon/PjtDataset/test_images/')\n",
        "\n",
        "# 훈련용 데이터와 시험용 데이터를 batch단위로 불러올 수 있게 \n",
        "# data_loader, test_data_loader를 각각 정의함\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=4, collate_fn=collate_fn)\n",
        "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "o5ASm9XU3o8K"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### S1-4P. 플젝코드 참고 : 커스텀 데이터셋 클래스 정의"
      ],
      "metadata": {
        "id": "ewylDCNUmcOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class CustomDataset(Dataset):\n",
        "#     def __init__(self, files, labels=None, mode='train'):\n",
        "#         self.mode = mode\n",
        "#         self.files = files\n",
        "#         self.csv_feature_dict = csv_feature_dict\n",
        "#         self.csv_feature_check = [0]*len(self.files)\n",
        "#         self.csv_features = [None]*len(self.files)\n",
        "#         self.max_len = 24 * 6\n",
        "#         self.label_encoder = label_encoder\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.files)\n",
        "    \n",
        "#     def __getitem__(self, i):\n",
        "#         file = self.files[i]\n",
        "#         file_name = file.split('/')[-1]\n",
        "        \n",
        "#         # csv\n",
        "#         if self.csv_feature_check[i] == 0:\n",
        "#             csv_path = f'{file}/{file_name}.csv'\n",
        "#             df = pd.read_csv(csv_path)[self.csv_feature_dict.keys()]\n",
        "#             df = df.replace('-', 0)\n",
        "#             # MinMax scaling\n",
        "#             for col in df.columns:\n",
        "#                 df[col] = df[col].astype(float) - self.csv_feature_dict[col][0]\n",
        "#                 df[col] = df[col] / (self.csv_feature_dict[col][1]-self.csv_feature_dict[col][0])\n",
        "#             # zero padding\n",
        "#             pad = np.zeros((self.max_len, len(df.columns)))\n",
        "#             length = min(self.max_len, len(df))\n",
        "#             pad[-length:] = df.to_numpy()[-length:]\n",
        "#             # transpose to sequential data\n",
        "#             csv_feature = pad.T\n",
        "#             self.csv_features[i] = csv_feature\n",
        "#             self.csv_feature_check[i] = 1\n",
        "#         else:\n",
        "#             csv_feature = self.csv_features[i]\n",
        "        \n",
        "#         # image\n",
        "#         image_path = f'{file}/{file_name}.jpg'\n",
        "#         img = cv2.imread(image_path)\n",
        "#         img = cv2.resize(img, dsize=(256, 256), interpolation=cv2.INTER_AREA)\n",
        "#         img = img.astype(np.float32)/255\n",
        "#         img = np.transpose(img, (2,0,1))\n",
        "        \n",
        "#         if self.mode == 'train':\n",
        "#             json_path = f'{file}/{file_name}.json'\n",
        "#             with open(json_path, 'r') as f:\n",
        "#                 json_file = json.load(f)\n",
        "            \n",
        "#             crop = json_file['annotations']['crop']\n",
        "#             disease = json_file['annotations']['disease']\n",
        "#             risk = json_file['annotations']['risk']\n",
        "#             label = f'{crop}_{disease}_{risk}'\n",
        "            \n",
        "#             return {\n",
        "#                 'img' : torch.tensor(img, dtype=torch.float32),\n",
        "#                 'csv_feature' : torch.tensor(csv_feature, dtype=torch.float32),\n",
        "#                 'label' : torch.tensor(self.label_encoder[label], dtype=torch.long)\n",
        "#             }\n",
        "#         else:\n",
        "#             return {\n",
        "#                 'img' : torch.tensor(img, dtype=torch.float32),\n",
        "#                 'csv_feature' : torch.tensor(csv_feature, dtype=torch.float32)\n",
        "#             }"
      ],
      "metadata": {
        "id": "SXF-NG3hmeqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### S1-4P. 플젝코드 참고 : 데이터로더 설정"
      ],
      "metadata": {
        "id": "59tESm3anpne"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ebv6SqTno8G"
      },
      "outputs": [],
      "source": [
        "# train = sorted(glob('data/train/*'))\n",
        "# test = sorted(glob('data/test/*'))\n",
        "\n",
        "# labelsss = pd.read_csv('data/train.csv')['label']\n",
        "# train, val = train_test_split(train, test_size=0.2, stratify=labelsss)\n",
        "\n",
        "# train_dataset = CustomDataset(train)\n",
        "# val_dataset = CustomDataset(val)\n",
        "# test_dataset = CustomDataset(test, mode = 'test')\n",
        "\n",
        "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=16, shuffle=True)\n",
        "# val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=16, shuffle=False)\n",
        "# test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 2. 모델 불러오기 : Faster R-CNN\n",
        "\n",
        "* 모델 개요\n",
        "    * 이번 마스크 검출의 경우에 사용된 모델은 Faster R-CNN모델에 기반한 Mask R-CNN모델\n",
        "\n",
        "    * Faster R-CNN모델은 이미지에 존재할 수 있는 객체에 대한 바운딩 박스와 클래스 점수를 모두 예측하는 모델\n",
        "\n",
        "    * Mask R-CNN 모델은 각 인스턴스에 대한 분할 마스크를 예측하는 추가 분기(레이어)를 Faster R-CNN에 추가한 모델\n",
        "\n",
        "* 전이학습(미리 학습된 모델을 가지고 와서 사용)\n",
        "    * 첫번째, 미리 학습된 모델에서 시작해서 마지막 레이어 수준만 미세 조정하는 방법\n",
        "    * 두번째, 모델의 백본을 다른 백본으로 교체하는 것. 일테면 더 빠른 예측을 하려고 할 때(예:ResNet101에서 MobilenetV2로 교체하면 수행 속도 향상 기대. 단, 인식 성능은 저하될 수 있음)\n",
        "\n",
        "* torchvision에서는 각종 컴퓨터 비전 문제를 해결하기 위한 딥러닝모델을 쉽게 불러올 수 있는 API를 제공함.\n",
        "* torchvision.model.detection에서는 Faster R-CNN API(torchvision.models.detection.fasterrcnn_resnet50_fpns)을 제공하고 있어 쉽게 구현이 가능 \n",
        "* 이는 COCO데이터셋을 ResNet50기반으로 pre-trained한 모델을 제공하고 있으며, pretrained=True/False로 설정할 수 있음\n",
        "\n",
        "* 이후 모델을 불러올 때는, \n",
        "* num_classes에 원하는 클래스 갯수를 설정하고 모델을 사용하면 됨\n",
        "* Faster R-CNN 사용 시 주의할 점은, background 클래스를 포함한 갯수를 num_classes에 명시해주어야 함. -> 실제 데이터셋의 클래스 갯수에 1개를 더해 backgrond 클래스 갯수를 추가해 주어야 함"
      ],
      "metadata": {
        "id": "A_ce3VbF3qZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 불러오는 함수 정의 : [retrain된 fasterrcnn_resnet50_fpn 모델\n",
        "# 미리 학습된 모델로부터 특정 클래스를 위해 미세 조정 \n",
        "def get_model_instance_segmentation(num_classes):\n",
        "    # COCO로 미리 학습된 모델 할당\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # 분류기를 새로운 것으로 교체하는데, num_classes는 사용자가 정의해 줌\n",
        "    # num_classes = 2  # 1 클래스(작물) + 배경\n",
        "\n",
        "    # 분류기에 넣어줘서 사용할 입력 특징의 차원 정보(갯수)를 얻음\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    # 마스크 예측기를(미리 학습된 모델의 머리 부분)을 새로운 것으로 교체함\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # 만일 모델을 일부 수정하려는 경우는 다음과 같이 사용함\n",
        "    '''\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    hidden_layer = 256\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, hidden_layer, num_classes)\n",
        "    '''\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "zoqrGNFR3zEt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 3. 전이 학습\n",
        "* FaceMaskDetection에 전이 학습을 실시해 봄\n",
        "* FaceMaskDetection 데이터셋은 3개의 클래스로 이루어져 있지만, background 클래스를 포함해 num_classes를 4로 설정한 후 모델을 불러와야 함\n",
        "* GPU를 사용할 수 있는 환경이라면 device로 지정해 불러온 모델을 GPU에 보내줌\n",
        "=>\n",
        "* 플젝(농작물 병해 진단)에 전이 학습을 실시해 봄\n",
        "* 플젝(농작물 병해 진단) 데이터셋은 1개의 클래스로 이루어져 있지만, background 클래스를 포함해 num_classes를 2로 설정한 후 모델을 불러오\n",
        "* GPU를 사용할 수 있는 환경이라면 device로 지정해 불러온 모델을 GPU에 보내줌"
      ],
      "metadata": {
        "id": "gLpeojPP54NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get_model_instance_segmentation()으로 모델 불러옴. 파라미터 4는 클래스의 수\n",
        "# 클래스 3개 + 배경 클래스 1개 = 4\n",
        "\n",
        "model = get_model_instance_segmentation(112)\n",
        "\n",
        "# GPU를 사용할 수 있으면 모델을 GPU에 보내줌\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "eaFFnYv65_RP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a14866fcdd9d4cc1ba50b41537577ac1",
            "9d65bf9eab0c47e280355e5a50f225b2",
            "3e5109930a8d474db70fbfc1acd84487",
            "2913c2b779114fbb83679f6d015bef93",
            "94152ce4713745b8bdc14170b33af432",
            "69add5ced3b840f690d82ca7b9df5cf6",
            "2d1212d6f2554b1eb9181eac60b0e8b6",
            "0ad94e67c1754a07aff79908505236b0",
            "b0f96e0726f04e8892ddd8bed5f039b2",
            "87e628b66ec34fbd82f4af10d070bb1d",
            "a75741d4a77f40e081db998028afdf18"
          ]
        },
        "outputId": "8bbb5497-e79b-48fa-a220-edf3760fa7d3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/160M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a14866fcdd9d4cc1ba50b41537577ac1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=112, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=448, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "7yFaHqIM6HdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc86c5d7-734b-46b5-ce1f-4ef489cc0067"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 4. 모델 학습"
      ],
      "metadata": {
        "id": "Sc36u0m0VJUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습. epoch지정해 주고, optimizer도 지정해 줌(여기선 SGD)\n",
        "\n",
        "num_epochs = 10\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                                momentum=0.9, weight_decay=0.0005)"
      ],
      "metadata": {
        "id": "MOAh-CMM6Oyk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_loader에서 한 배치씩 순서대로 모델에 사용하며, 이후 loss 계산을 통해 최적화 수행\n",
        "# 각 epoch마다 출력되는 loss를 통해 학습이 진행되는 것을 확인할 수 있음\n",
        "\n",
        "print('----------------------train start--------------------------')\n",
        "for epoch in range(num_epochs):\n",
        "    start = time.time()\n",
        "    model.train()\n",
        "    i = 0    \n",
        "    epoch_loss = 0\n",
        "    for imgs, annotations in data_loader:\n",
        "        i += 1\n",
        "        imgs = list(img.to(device) for img in imgs)\n",
        "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
        "\n",
        "        ##################################################################\n",
        "        # print(len(imgs))\n",
        "        # print(len(annotations))\n",
        "\n",
        "        # 1, 2, 3, 4\n",
        "        # [1, 2, 3, 4] <- list라서 tensor의 차원이 없음\n",
        "        # torch.cat([1, 2, 3, 4], dim=0) <- (N,) Tensor의 차원이 생김\n",
        "        # annotations = torch.cat(annotations, dim=0) # batch dimension 추가\n",
        "        ##################################################################\n",
        "\n",
        "        loss_dict = model(imgs, annotations) \n",
        "        losses = sum(loss for loss in loss_dict.values())        \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step() \n",
        "        epoch_loss += losses\n",
        "    print(f'epoch : {epoch+1}, Loss : {epoch_loss}, time : {time.time() - start}')"
      ],
      "metadata": {
        "id": "cvGheWhe6TPn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3358c79-bc4d-49fe-8422-474edc44f2e4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------train start--------------------------\n",
            "epoch : 1, Loss : 235.722900390625, time : 4257.989573478699\n",
            "epoch : 2, Loss : 126.26966857910156, time : 554.8524603843689\n",
            "epoch : 3, Loss : 94.87674713134766, time : 555.0279641151428\n",
            "epoch : 4, Loss : 81.01116180419922, time : 553.6131205558777\n",
            "epoch : 5, Loss : 72.16987609863281, time : 553.2636377811432\n",
            "epoch : 6, Loss : 65.8717269897461, time : 552.6955242156982\n",
            "epoch : 7, Loss : 62.49577331542969, time : 552.5369303226471\n",
            "epoch : 8, Loss : 60.7906608581543, time : 552.6625919342041\n",
            "epoch : 9, Loss : 59.82156753540039, time : 552.6528086662292\n",
            "epoch : 10, Loss : 58.31047439575195, time : 552.6728663444519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save() : 학습시킨 가중치를 저장해 두고 나중에 언제든지 불러와 사용\n",
        "torch.save(model.state_dict(),f'model_{num_epochs}.pt')"
      ],
      "metadata": {
        "id": "rBKYjjB66WHb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "e51954ce-9da8-4981-ad1a-647fcf5218ce"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a392637ffcac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# torch.save() : 학습시킨 가중치를 저장해 두고 나중에 언제든지 불러와 사용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf'model_{num_epochs}.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(f'model_{num_epochs}.pt'))"
      ],
      "metadata": {
        "id": "6JSnhMJQ6Y2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 5. Inference\n",
        "* 모델 학습이 끝났으면 잘 학습되었는지 예측 결과를 확인해 봄\n",
        "* 예측결과에는 바운딩박스의 좌표(bbox)와 클래스(label), 점수(scores)가 포함됨\n",
        "* 점수(scores)에는 해당 클래스의 신뢰도 값이 저장되는데..\n",
        "* make_prediction : treshhold값이 0.5이상인 것만 추출하도록 설정해주는 파라미터\n",
        "* test_data_loader의 첫번째 배치에 대해서만 결과 출력"
      ],
      "metadata": {
        "id": "EerOoaPR6lBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction(model, img, threshold):\n",
        "    model.eval()\n",
        "    preds = model(img)\n",
        "    for id in range(len(preds)) :\n",
        "        idx_list = []\n",
        "\n",
        "        for idx, score in enumerate(preds[id]['scores']) :\n",
        "            if score > threshold : \n",
        "                idx_list.append(idx)\n",
        "\n",
        "        preds[id]['labels'] = preds[id]['labels'][idx_list]\n",
        "        preds[id]['boxes'] = preds[id]['boxes'][idx_list]\n",
        "\n",
        "        ''' 이 부분은 일단 배제\n",
        "        preds[id]['pboxes'] = preds[id]['pboxes'][idx_list]\n",
        "        ''' \n",
        "        \n",
        "        preds[id]['scores'] = preds[id]['scores'][idx_list]\n",
        "\n",
        "    return preds"
      ],
      "metadata": {
        "id": "pssASrXy6r6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): \n",
        "    # 테스트셋 배치사이즈= 2\n",
        "    for imgs, annotations in test_data_loader:\n",
        "        imgs = list(img.to(device) for img in imgs)\n",
        "\n",
        "        pred = make_prediction(model, imgs, 0.5)\n",
        "        print(pred)\n",
        "        break"
      ],
      "metadata": {
        "id": "IBeD60JJ6vXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측된 결과를 이용해 이미지 위에 바운딩 박스를 그려봄\n",
        "# 위에서 정의한 plot_image_from_output()함수로 그림을 출력함\n",
        "# Target이 실제 바운딩 박스 위치,, Prediction이 모델이 예측한 결과\n",
        "\n",
        "_idx = 1\n",
        "\n",
        "# 실제 바운딩 박스 위치\n",
        "print(\"Target : \", annotations[_idx]['labels'])\n",
        "plot_image_from_output(imgs[_idx], annotations[_idx])\n",
        "\n",
        "# 모델이 예측한 바운딩 박스 위치 결과\n",
        "print(\"Prediction : \", pred[_idx]['labels'])\n",
        "plot_image_from_output(imgs[_idx], pred[_idx])"
      ],
      "metadata": {
        "id": "ASZ25gx46vcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 6. 평가\n",
        "* 전체 시험 데이터에 대해 예측 결과를 평가해 봄\n"
      ],
      "metadata": {
        "id": "W3AFacoCVZ7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### S6-1. 모든 시험 데이터에 대한 예측 결과와 실제 label을 각각 preds_adj_all, annot_all에 담아줌"
      ],
      "metadata": {
        "id": "UYQGanR_VzzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 시험 데이터에 대한 예측 결과와 실제 label을 \n",
        "# 각각 preds_adj_all, annot_all에 담아줌\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "labels = []\n",
        "preds_adj_all = []\n",
        "annot_all = []\n",
        "\n",
        "for im, annot in tqdm(test_data_loader, position = 0, leave = True):\n",
        "    im = list(img.to(device) for img in im)\n",
        "    #annot = [{k: v.to(device) for k, v in t.items()} for t in annot]\n",
        "\n",
        "    for t in annot:\n",
        "        labels += t['labels']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        preds_adj = make_prediction(model, im, 0.5)\n",
        "        preds_adj = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in preds_adj]\n",
        "        preds_adj_all.append(preds_adj)\n",
        "        annot_all.append(annot)"
      ],
      "metadata": {
        "id": "m01hmSr96vhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### S6-2. mAP값을 산출\n",
        "* Tutorial-Book-Utils 폴더 내에 있는 utils_ObejctDetection.py 파일을 통해 mAP값을 산출"
      ],
      "metadata": {
        "id": "7zCpfuJVVsrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Tutorial-Book-Utils/\n",
        "import utils_ObjectDetection as utils"
      ],
      "metadata": {
        "id": "lqsiSosc6vlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get_batch_statistics 함수를 통해 IoU(Intersection of Union) 조건을 만족하는 \n",
        "# 바운딩 박스간의 통곗값을 계산후\n",
        "sample_metrics = []\n",
        "for batch_i in range(len(preds_adj_all)):\n",
        "    sample_metrics += utils.get_batch_statistics(preds_adj_all[batch_i], annot_all[batch_i], iou_threshold=0.5) \n",
        "\n",
        "# ap_per_class 함수를 통해 각 클래스에 대한 AP값을 계산해줍니다.\n",
        "true_positives, pred_scores, pred_labels = [torch.cat(x, 0) for x in list(zip(*sample_metrics))]  # 배치가 전부 합쳐짐\n",
        "precision, recall, AP, f1, ap_class = utils.ap_per_class(true_positives, pred_scores, pred_labels, torch.tensor(labels))\n",
        "mAP = torch.mean(AP)\n",
        "print(f'mAP : {mAP}')\n",
        "print(f'AP : {AP}')"
      ],
      "metadata": {
        "id": "5rbrNBnB6vpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 회고\n",
        "\n",
        "* AP값은 background 클래스를 제외한 실제 3개의 클래스에 대해서만 보여줍니다. \n",
        "\n",
        "* 10번만 학습했음에도 불구하고 4장의 RetinaNet 결과보다 향상된 것을 확인할 수 있습니다. \n",
        "\n",
        "* 특히나 1번 클래스인 마스크 착용 객체에 대해서는 0.9189 AP에 해당하는 정확도까지 보이고 2번 클래스인 마스크를 제대로 착용하고 있지 않는 객체에서도 0.3664 AP를 보이고 있습니다. \n",
        "\n",
        "* RetinaNet이 FPN과 Focal loss로 one-stage method임에도 높은 성능을 보인다고 일반적으로 알려져 있습니다. \n",
        "\n",
        "* 물론 하이퍼파라미터 튜닝을 통해 RetinaNet의 성능을 최적화 해도 되겠지만, 현재 실험 결과로 미뤄봤을 때 이 데이터셋에는 Faster-RCNN이 더 좋은 성능을 보이고 있습니다."
      ],
      "metadata": {
        "id": "QhEUKixX6vtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NFA7cdzhXNQq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}